{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Binary Detection Template\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kornia\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from astropy.io import fits\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import models\n",
    "from typing import Tuple, List\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## FLAGS\n",
    "\n",
    "Change the following to {`True`, `False`} to enable or disable the execution of generating .csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "RECREATE_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "### Generate CSV files\n",
    "\n",
    "The following generates `.csv` files for training, validation and testing, based on `.fits` files placed in `binary` and `else` folders, and labels them `1` and `0`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_combined_csv(root_folder: str, output_folder: str, output_file: str = 'combined.csv'):\n",
    "    \"\"\"\n",
    "    Creates a combined CSV file from .fits files in the given folders.\n",
    "    \n",
    "    Parameters:\n",
    "    root_folder (str): The root folder containing the 'binary' and 'else' folders.\n",
    "    output_folder (str): The folder where the CSV files will be saved.\n",
    "    output_file (str): The name of the output combined CSV file.\n",
    "    \"\"\"\n",
    "    # Define the subfolders and corresponding labels\n",
    "    folders = {'binary': 1, 'else': 0}\n",
    "    data = []\n",
    "\n",
    "    # Iterate over the folders and files\n",
    "    for folder, label in folders.items():\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            # Consider only FITS files\n",
    "            if file.endswith('.fits'):\n",
    "                # Save the absolute path, whith the corresponding label\n",
    "                data.append({'path': os.path.abspath(os.path.join(folder_path, file)), 'label': label})\n",
    "\n",
    "    # Create a DataFrame and save as CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    df.to_csv(os.path.join(output_folder, output_file), index=False)\n",
    "\n",
    "\n",
    "def stratified_split(input_df: pd.DataFrame,\n",
    "                     train_size: int = 0.7,\n",
    "                     test_size: int = 0.15,\n",
    "                     random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Performs a stratified split on the input DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    input_df (DataFrame): The input DataFrame to be split.\n",
    "    train_size (float): The proportion of the dataset to include in the train split.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): The seed used by the random number generator.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the train, validation, and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Split the data into training and temporary (validation + test)\n",
    "    train_df, temp_df = train_test_split(input_df, train_size=train_size, stratify=input_df['label'],\n",
    "                                         random_state=random_state)\n",
    "\n",
    "    # Split the temp data into validation and test\n",
    "    val_size = test_size / (1 - train_size)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=val_size, stratify=temp_df['label'],\n",
    "                                       random_state=random_state)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def save_csv_files(dfs: List[pd.DataFrame],\n",
    "                   output_folder: str,\n",
    "                   filenames: List[str] = ['train.csv', 'valid.csv', 'test.csv']):\n",
    "    \"\"\"\n",
    "    Saves given DataFrames as CSV files in the specified folder.\n",
    "    \n",
    "    Parameters:\n",
    "    dfs (list of DataFrame): List of DataFrames to be saved.\n",
    "    output_folder (str): The folder where the CSV files will be saved.\n",
    "    filenames (list of str): Names of the output CSV files.\n",
    "    \"\"\"\n",
    "    for df, filename in zip(dfs, filenames):\n",
    "        df.to_csv(os.path.join(output_folder, filename), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Run the following lines only if there has been a change in the dataset, so you want to recreate the `.csv` files.\n",
    "Once you have created the `.csv` files, you can set the `RECREATE_DATASET` flag to `False` so that it does not do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "root_folder = 'dataset'\n",
    "output_folder = os.path.join('dataset', 'csv')\n",
    "\n",
    "if RECREATE_DATASET:\n",
    "    # Create combined CSV\n",
    "    create_combined_csv(root_folder, output_folder)\n",
    "\n",
    "    # Load the combined CSV\n",
    "    combined_df = pd.read_csv(os.path.join(output_folder, 'combined.csv'))\n",
    "\n",
    "    # Perform stratified split\n",
    "    train_df, val_df, test_df = stratified_split(combined_df)\n",
    "\n",
    "    # Save the split datasets\n",
    "    save_csv_files([train_df, val_df, test_df], output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dataset and Data Augmentations\n",
    "\n",
    "#### Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class FitsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for handling .fits files.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing paths and labels.\n",
    "        transform (callable, optional): Optional transform (data augmentation) to be applied on a sample.\n",
    "\n",
    "    Attributes:\n",
    "        data_frame (DataFrame): Pandas DataFrame containing the file paths and labels.\n",
    "        transform (callable): Transform to be applied on a sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file: str, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fits_path = self.data_frame.iloc[idx, 0]  # first column of .csv file\n",
    "        label = self.data_frame.iloc[idx, 1]  # second column of .csv file\n",
    "\n",
    "        with fits.open(fits_path) as hdul:\n",
    "            image_data = hdul[0].data\n",
    "\n",
    "        # Normalize the image data to [0,1]\n",
    "        min_val = np.min(image_data)\n",
    "        max_val = np.max(image_data)\n",
    "        image_data = (image_data - min_val) / (max_val - min_val)\n",
    "        image_data = image_data.astype(np.float32)\n",
    "        \n",
    "        # Create Tensor\n",
    "        image_data = torch.from_numpy(image_data)\n",
    "        # add channel dimension\n",
    "        image_data = image_data.unsqueeze(0)\n",
    "        \n",
    "        # Apply data augmentation if defined\n",
    "        if self.transform:\n",
    "            image_data = self.transform(image_data)\n",
    "\n",
    "        # Center crop to size\n",
    "        image_data = transforms.functional.center_crop(image_data, output_size=(224, 224))\n",
    "        \n",
    "        return image_data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Define Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CustomTransform:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.angles = [0, 90, 180, 270]\n",
    "        self.translation_values = [-1, 0, 1]\n",
    "\n",
    "    def __call__(self, input_data):\n",
    "\n",
    "        # Random horizontal flip with probability p\n",
    "        if random.random() < self.p:\n",
    "            input_data = torch.flip(input_data, [-1])\n",
    "\n",
    "        # Random vertical flip with probability p\n",
    "        if random.random() < self.p:\n",
    "            input_data = torch.flip(input_data, [-2])\n",
    "\n",
    "        # Random translation with one of the values\n",
    "        trans_x = random.choice(self.translation_values)\n",
    "        trans_y = random.choice(self.translation_values)\n",
    "        translation_value = torch.tensor([[trans_x, trans_y]], dtype=torch.float32)\n",
    "        input_data = kornia.geometry.transform.translate(input_data, translation_value, mode='nearest')\n",
    "\n",
    "        # Random rotation with one of the angles\n",
    "        angle = random.choice(self.angles)\n",
    "        angle = torch.tensor(angle, dtype=torch.float32)\n",
    "        # # Add batch dimension\n",
    "        # input_data = input_data.unsqueeze(0)\n",
    "        input_data = kornia.geometry.transform.rotate(input_data, angle, mode='bicubic')\n",
    "\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Let's visualize the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_csv = os.path.join('dataset', 'csv', 'train.csv')\n",
    "valid_csv = os.path.join('dataset', 'csv', 'valid.csv')\n",
    "\n",
    "custom_transform = CustomTransform(p=0.5)\n",
    "\n",
    "train_dataset = FitsDataset(train_csv, custom_transform)\n",
    "valid_dataset = FitsDataset(valid_csv)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Fetch the first batch\n",
    "first_batch_images, first_batch_labels = next(iter(train_loader))\n",
    "\n",
    "# Create a 4x4 grid for subplot\n",
    "fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
    "\n",
    "# Flatten the array of axes for easy iteration\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(16):\n",
    "    # Plot each image in its respective subplot\n",
    "    axs[i].imshow(first_batch_images[i].squeeze(), cmap='gray')\n",
    "    axs[i].set_title(f\"Label: {first_batch_labels[i]}\")\n",
    "    axs[i].axis('off')  # Turn off axis to make the plot cleaner\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "\n",
    "    # Load ResNet model without pre-trained weights\n",
    "    model = models.resnet34(pretrained=False)\n",
    "    # Change input channels from 3 to 1\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    # Change output channels to 2 (binary classification)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.05)\n",
    "\n",
    "    # Save the best model to this path\n",
    "    best_model_path = os.path.join(\"trained_models\")\n",
    "    os.makedirs(best_model_path, exist_ok=True)\n",
    "    best_model = os.path.join(best_model_path, f'best_model.pth')\n",
    "\n",
    "    # Check if GPU is available and move the model to GPU if it is\n",
    "    # Specify the GPU to use (e.g., GPU 0)\n",
    "    target_gpu = 0\n",
    "    device = torch.device(f\"cuda:{target_gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(target_gpu)}\")\n",
    "\n",
    "    #device = torch.device(gip if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    num_epochs = 40  # Number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # training the model\n",
    "    best_f1 = 0.0\n",
    "    for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "        model.train()\n",
    "        train_preds, train_targets = [], []\n",
    "        train_losses = []\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_targets.extend(labels.cpu().numpy())\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        train_f1 = f1_score(train_targets, train_preds, average='weighted')\n",
    "\n",
    "        model.eval()\n",
    "        valid_preds, valid_targets = [], []\n",
    "        valid_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                valid_preds.extend(preds.cpu().numpy())\n",
    "                valid_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "        valid_f1 = f1_score(valid_targets, valid_preds, average='weighted')\n",
    "\n",
    "        if valid_f1 >= best_f1:\n",
    "            best_f1 = valid_f1\n",
    "            torch.save(model.state_dict(), best_model)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train F1: {train_f1:.4f}, Valid F1: {valid_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Let's test our trained model!\n",
    "\n",
    "### Define test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    \n",
    "    test_csv = os.path.join('dataset', 'csv', 'test.csv')\n",
    "    test_dataset = FitsDataset(test_csv)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Load the best trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(best_model))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Let's see how it works on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    test_preds, test_targets = [], []\n",
    "    test_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "    # Generate a classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(test_targets, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Production Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionFitsDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for handling .fits files.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing paths and labels.\n",
    "\n",
    "    Attributes:\n",
    "        data_frame (DataFrame): Pandas DataFrame containing the file paths and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file: str):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fits_path = self.data_frame.iloc[idx, 0]\n",
    "\n",
    "        with fits.open(fits_path) as hdul:\n",
    "            image_data = hdul[0].data\n",
    "\n",
    "        # Normalize the image data to [0,1]\n",
    "        min_val = np.min(image_data)\n",
    "        max_val = np.max(image_data)\n",
    "        image_data = (image_data - min_val) / (max_val - min_val)\n",
    "        image_data = image_data.astype(np.float32)\n",
    "\n",
    "        # Create Tensor\n",
    "        image_data = torch.from_numpy(image_data)\n",
    "        # add channel dimension\n",
    "        image_data = image_data.unsqueeze(0)\n",
    "\n",
    "        # Center crop to size\n",
    "        image_data = transforms.functional.center_crop(image_data, output_size=(224, 224))\n",
    "\n",
    "        return image_data\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Production CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_production_csv(root_folder: str, output_folder: str, output_file: str = 'production.csv'):\n",
    "    \"\"\"\n",
    "    Creates a combined CSV file from .fits files in the given folder.\n",
    "\n",
    "    Parameters:\n",
    "    root_folder (str): The root folder containing the FITS files.\n",
    "    output_folder (str): The folder where the CSV file will be saved.\n",
    "    output_file (str): The name of the output CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    root_folder = '/dataplus3/ilknur/stamp_images/binary_stamps/panstarr/panstarr_stacked/fits'\n",
    "    output_folder = '/home/jupyter-ilknur/BinaryDetection/production'\n",
    "    \n",
    "    prod_data = []\n",
    "\n",
    "    # Iterate over the files in the root folder\n",
    "    for file in os.listdir(root_folder):\n",
    "        # Consider only FITS files\n",
    "        if file.endswith('.fits'):\n",
    "            # Save the absolute path\n",
    "            prod_data.append({'path': os.path.abspath(os.path.join(root_folder, file))})\n",
    "            \n",
    "    # Create a DataFrame and save as CSV\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    prod_df = pd.DataFrame(prod_data)\n",
    "    prod_df.to_csv(os.path.join(output_folder, output_file), index=False)\n",
    "    \n",
    "create_production_csv(root_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Production Dataset and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "prod_csv = ('/home/jupyter-ilknur/BinaryDetection/production/production.csv')\n",
    "prod_dataset = ProductionFitsDataset(prod_csv)\n",
    "prod_loader = DataLoader(prod_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Define and load your model\n",
    "#best_model_path = os.path.join('/home/jupyter-ilknur/BinaryDetection/trained_models')\n",
    "#best_model = os.path.join(best_model_path, f'best_model.pth')\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(best_model))\n",
    "model.eval()\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "# Make predictions\n",
    "    \n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in prod_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "            \n",
    "\n",
    "# Now 'predictions' contains the predicted labels for your production dataset\n",
    "#print(predictions)\n",
    "\n",
    "# Save predictions to CSV\n",
    "pred_df = pd.DataFrame({'Predictions': predictions})\n",
    "pred_df.to_csv('/home/jupyter-ilknur/BinaryDetection/production/predictions.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Production Dataset with the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = pd.read_csv('/home/jupyter-ilknur/BinaryDetection/production/production.csv')\n",
    "df1 = pd.DataFrame(T1)\n",
    "T2 = pd.read_csv('/home/jupyter-ilknur/BinaryDetection/production/predictions.csv')\n",
    "df2 = pd.DataFrame(T2)\n",
    "result = pd.concat([df1, df2], axis=1)\n",
    "result.to_csv('/home/jupyter-ilknur/BinaryDetection/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
